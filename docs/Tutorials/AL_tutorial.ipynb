{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "503d3bae",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/petzschnerlab/avoid_learning_analysis/blob/main/docs/Tutorials/AL_tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4520c3d",
   "metadata": {},
   "source": [
    "# AL Analysis Tutorial\n",
    "\n",
    "Welcome to the avoidance learning analysis repo. This repo was built for the PEAC lab to analyse behavioural data from the various avoidance learning tasks. This repo loads the data, conducts statistics on the data, plots the data, and generates a report as a PDF file, which presents the main findings of the study.\n",
    "\n",
    "*Note that this tutorial is designed to run in Google Colab and not from the repo itself (since it clones the repo)*\n",
    "\n",
    "## Project Pipeline\n",
    "This repo is one part of a project pipeline, which requires the coordination of multiple repos. Projects begin with a <b>task repo</b>, which is used to collect behavioural data from participants either locally or on Prolific. The collected data must then be pushed through a <b>data extraction repo</b> to prepare CSV files for analysis. These CSV files are used in <b>the analysis repo (this repo)</b>, which creates a PDF report (`AL/reports`), ending the project pipeline. \n",
    "\n",
    "Optionally, you can run computational reinforcement learning models using the <b>modelling repo</b>, and the results can be added to the report here. This is a bit clunky because it requires a bit of back-and-forth between this repo and the modelling repo. Specifically, this repo must be run (with `load_models=False`, see [Parameters](HowToUse/parameters.md) in documentation) in order to create two CSV files that the modelling repo needs (`AL/data/pain_learning_processed.csv` and `AL/data/pain_transfer_processed.csv`). These files can then be manually moved into the modelling repo's data directory (`RL/data`). The modelling repo can then be used to model the data, which will result in a newly constructed directory called `modelling` (`RL/modelling`). This folder can then be manually moved to this analysis repo as `AL/modelling`. Then you can re-run this repo (with `load_models=True`) and the modelling results will be included in the PDF report. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4702b22f",
   "metadata": {},
   "source": [
    "## Cloning the Repo\n",
    "\n",
    "We will begin by cloning the repo, installing dependencies, and then adding this repo as a system path. Adding the repo in the system path is only necessary for this tutorial. We also change directory to the repo. When using locally, you can create your script in the `AL` source folder, in the same manner as `AL_main.py` (`avoid_learning_analysis/AL/AL_main.py`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c10e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# We will now clone the repo, pull any updates, and install dependencies\n",
    "!git clone https://github.com/petzschnerlab/avoid_learning_analysis.git\n",
    "%cd avoid_learning_analysis/\n",
    "!git pull\n",
    "!pip install .\n",
    "\n",
    "#Only necessary for Google Colab\n",
    "sys.path.insert(0, os.path.abspath(\"/content/avoid_learning_analysis/AL\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2edb154",
   "metadata": {},
   "source": [
    "## The Pipeline\n",
    "\n",
    "Next, we will import the Pipeline class. This class is the entry point to this repo. It will take in all of your parameters and run the corresponding analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f88e700",
   "metadata": {},
   "source": [
    "### The Help Function\n",
    "\n",
    "The pipeline has a help function that will outline some information about the repo and then describe all of the parameters. These details are also available in the documentation. We will use the `help=True` parameters in order to see this help function below. \n",
    "\n",
    "This parameter can be passed to the Pipeline during initiatialization:\n",
    "```\n",
    "pipeline = Pipeline(help=True)\n",
    "```\n",
    "\n",
    "or to the pipeline run method of the class:\n",
    "```\n",
    "pipeline = Pipeline()\n",
    "pipeline.run(help=True)\n",
    "```\n",
    "\n",
    "The help information gets truncated in Jupyter notebooks, but you can view the whole output by clicking `scrollable element`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53acdf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(help=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc529c0",
   "metadata": {},
   "source": [
    "## Running the Pipeline\n",
    "\n",
    "Running the pipeline requires inputting parameters to the run method. For this package, there two required parameters, `file_path` and `file_name`. \n",
    "\n",
    "`file_path`: Path to the data file(s) to be loaded. From this path, you can load several different files using the file_name parameter.\n",
    "`file_name`: Name of the file(s) to be loaded. These filenames should be relative to the file_path parameter. You can load multiple files by providing a list of file names or a single file name as a string. You can add further path information here if your data splits at the point of file_path. For example, file_path = \"path/to/data\" and file_name = [\"subfolder1/data1.csv\", \"subfolder2/data2.csv\"] will load two files from different subfolders.\n",
    "\n",
    "We will define a typical set of parameters for this package below, see the help information above to understand what each parameters does.\n",
    "\n",
    "Processing the data will take a bit of time, so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df40eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture \n",
    "\n",
    "params = {\n",
    "    'author':                         'Chad C. Williams',\n",
    "\n",
    "    'file_path':                      os.path.join('AL','data'),\n",
    "    'file_name':                      'tutorial_data.csv',\n",
    "\n",
    "    'accuracy_exclusion_threshold':   70, #Exclusion threshold for accuracy\n",
    "    'RT_low_threshold':               200, #Lower exclusion threshold for RT\n",
    "    'RT_high_threshold':              5000, #Upper exclusion threshold for RT\n",
    "    \n",
    "    'load_stats':                     True, #Run stats on data\n",
    "    'load_posthocs':                  True, #Run posthoc tests on data\n",
    "    'hide_posthocs':                  True, #Hide posthoc results from the report\n",
    "}\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.run(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9ad5e",
   "metadata": {},
   "source": [
    "## The Report\n",
    "\n",
    "The report is saved as a PDF and displays the main findings of the analyses. You can find this PDF under `AL/reports/PEAC_report_pain.pdf` (this is the default name, but it can be changed with the `print_filename` parameter). We will also display it below, but it's best to view this report directly, so navigate to the report and see your findings! \n",
    "\n",
    "Keep in mind that the tutorial data only contains five participants per group, so our plots and statistics will not look to great in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "pdf_path = \"AL/reports/PEAC_report_pain.pdf\"\n",
    "\n",
    "with open(pdf_path, \"rb\") as f:\n",
    "    pdf_bytes = f.read()\n",
    "    encoded = base64.b64encode(pdf_bytes).decode(\"utf-8\")\n",
    "\n",
    "pdf_display = f'<embed src=\"data:application/pdf;base64,{encoded}\" width=\"700\" height=\"900\" type=\"application/pdf\">'\n",
    "\n",
    "HTML(pdf_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f17d61",
   "metadata": {},
   "source": [
    "Although the report gives you a general overview of all findings, you may want to look at the files used to build it more directly. Let's begin by observing the participant pain scores across the groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f358f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (4116336026.py, line 6)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m'Pain metrics for each group. Boxplots show the mean and 95% confidence intervals of the corresponding metric for each group. '\u001b[39m\n    ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(filename='RL/plots/pain/demo-clinical-scores.png'))\n",
    "\n",
    "caption = (\n",
    "    'Pain metrics for each group. Boxplots show the mean and 95% confidence intervals of the corresponding metric for each group. '\n",
    "    'Half-violin plots show the distribution of the scores of the corresponding metric for each group. '\n",
    "    'Scatter points show the scores of the corresponding metric for each participant within each group.'\n",
    ")\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f40500",
   "metadata": {},
   "source": [
    "Next, we can view the behavioural data for both the learning and transfer phases across our groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75563342",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='RL/plots/pain/empirical-performance.png'))\n",
    "caption = (\n",
    "    'Empirical findings of learning accuracy and transfer choice rates. '\n",
    "    'a. Learning Phase: Behavioral performance across binned learning trials for the reward and punishment contexts for each group. Shaded regions represent 95% confidence intervals. '\n",
    "    'b. Transfer Phase: Choice rates for each stimulus type during transfer trials for each group. '\n",
    "    'Choice rate is computed as the percentage of times a stimulus type was chosen, given the number of times it was presented. '\n",
    "    'Bar plots show the mean and 95% confidence intervals of the choice rate for each stimulus type across participants within each group. '\n",
    "    'Abbreviations: HR – high reward rate (75% reward), LR – low reward rate (25% reward), LP – low punishment rate (25% loss), HP – high punishment rate (75% loss), N - novel stimulus.'\n",
    ")\n",
    "print(caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607e7d0",
   "metadata": {},
   "source": [
    "Let's look at some statistics next. We can see the results for our GLMMs (or multiple regression if not using R) for all of our analyses. Let's focus on the choice rates in the transfer phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9946e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "glmm_results = pd.read_csv('AL/stats/pain_stats_choice_rates_choice_rate_results.csv')\n",
    "glmm_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c959e76",
   "metadata": {},
   "source": [
    "## Computational Modelling of Empirical Data (Optional)\n",
    "\n",
    "Now that we are done data analysis, you might want to proceed to computationally modelling the data using our modelling repo. This repo requires two files that the run function built for us, specifically `AL/data/pain_learning_processed.csv` and `AL/data/pain_transfer_processed.csv`. This tutorial will end here, but if you want to continue with computational modelling, go to the tutorial in the modelling repo and it will contain these data for you to continue with. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SOMA_AL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
